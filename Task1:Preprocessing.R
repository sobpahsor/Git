
#Read the file into the r and store it as reuters
reuters <- read.csv("D:/reutersCSV.csv/reutersCSV.csv")

#Due to the reason that this package has been removed from the CRAN website, we need to 
install.packages("http://datacube.wu.ac.at/src/contrib/openNLPmodels.en_1.5-1.tar.gz", repos=NULL, type = "source");

#-------------------------------------
library(tm)
library(koRpus)
library(openNLP)
library(openNLPmodels.en)
library(NLP)
#-------------------------------------
#Please run these two functions prior to the preprocessing and save the data frame as reuters
ClearZeroColumn<-function(a){
  v<-c(0)
  for(i in 4:138){
    x<-sum(a[,i])
    if(x == 0)v<-append(v,i,after = length(v))
  }
  v<-v[-1]
  clearzero<-a[,-v]
  return(data.frame(clearzero))
}

ClearZeroRow<-function(a){
        v<-c(0)
        for(i in 1:21578){
           x<-sum(a[i,4:121])
           if(x == 0)v<-append(v,i,after = length(v))
         }
       v<-v[-1]
       clearzero<-a[-v,]
       return(data.frame(clearzero))
}

#find the entity from the Maxent_entity_annotator function in openNLP library
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
person_entity <- Maxent_Entity_Annotator(kind = "person")
location_entity <- Maxent_Entity_Annotator(kind = "location")
organization_entity <- Maxent_Entity_Annotator(kind = "organization")
date_entity <- Maxent_Entity_Annotator(kind = "date")
money_entity <- Maxent_Entity_Annotator(kind = "money")
percentage_entity<- Maxent_Entity_Annotator(kind = "percentage")

reuters[,"processed_text"]<-NA
reuters[,"processed_text_copy"]<-NA

for(j in 1:11340){
  print(j)
  if(reuters[j,123] != ""){
    
    #Tokenize
    char<-as.character(reuters[j,123])
    char<-gsub("/"," ",char)
    tokenchar<-scan_tokenizer(char)
    
    #Remove Punctuation
    puncRemoveChar <- removePunctuation(tokenchar, preserve_intra_word_dashes = TRUE)
    
    #词性识别
    Schar<-paste(puncRemoveChar, collapse=" ")
    Schar<-as.String(Schar)
    anno <- annotate(Schar, list(sent_token_annotator, word_token_annotator))
    anno_pos<-annotate(Schar,pos_tag_annotator,anno)
    
    #lemmatization
    #install.packages("koRpus")
    #Download the Treetagger from (http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) to your working space(getwd() function can get the path)
    #Download the english parameter file to the lib folder in Treetagger and change the name to english.par
    lemma_res <- treetag(puncRemoveChar, treetagger="manual", 
                         format="obj", TT.tknz=FALSE , lang="en", 
                         TT.options=list(path="./TreeTagger", preset="en"))
    lemma_tab <- lemma_res@TT.res[1:3]
    lemma_tab[which(lemma_tab[,3] == "@card@"),3]<- puncRemoveChar[which(lemma_tab[,3] == "@card@")]
    lemma_tab[which(lemma_tab[,3] == "<unknown>"),3]<-puncRemoveChar[which(lemma_tab[,3] == "<unknown>")]
    lemma_tab<-lemma_tab[,-2]
    
    #Remove stop words
    #从POS中找到需要去除的词性
    annow<-subset(anno_pos, type == "word")
    poslist<-sapply(annow$features,'[[',"POS")
    
    stopLineNum<-c(which(poslist == "PRP"),which(poslist == "PRP$"),
                   which(poslist == "IN"),which(poslist == "TO"),
                   which(poslist == "MD"),which(poslist == "DT"),
                   which(poslist == "WDT"),which(poslist == "VBP"),which(poslist == "CC"))
    if(length(stopLineNum) != 0){
      AfterStop<-lemma_tab[-stopLineNum,]
      
      #将单词连起来再次形成一个text
      txt<-paste(as.vector(AfterStop[,2]), collapse = " ")
      stxt<-as.String(txt)
      #Name Entity Substitution
      aaa <- annotate(stxt, list(sent_token_annotator, word_token_annotator))
      
      per_en<-person_entity(stxt,aaa)
      if(length(per_en) != 0)perName<-stxt[per_en]
      
      loc_en<-location_entity(stxt,aaa)
      if(length(loc_en) != 0)locName<-stxt[loc_en]
      
      org_en<-organization_entity(stxt,aaa)
      if(length(org_en) != 0)orgName<-stxt[org_en]
      
      dat_en<-date_entity(stxt,aaa)
      if(length(dat_en) != 0)datName<-stxt[dat_en]
      
      mon_en<-money_entity(stxt,aaa)
      if(length(mon_en) != 0)monName<-stxt[mon_en]
      
      percent_en<-percentage_entity(stxt,aaa)
      if(length(percent_en) != 0)percentName<-stxt[percent_en]
      
      if(length(per_en) != 0){
        if(length(per_en) == 1)stxt<-gsub(perName, "PERSON", stxt)
        else {for(i in 1:length(perName)){
              stxt<-gsub(perName[i], "PERSON", stxt)
            }
        }
      }
      if(length(loc_en) != 0){
        if(length(loc_en) == 1)stxt<-gsub(locName, "LOCATION", stxt)
        else {for(i in 1:length(locName)){
              stxt<-gsub(locName[i], "LOCATION", stxt)
            }
        }
      }
      
      if(length(org_en) != 0){
        if(length(org_en) == 1)stxt<-gsub(orgName, "ORG", stxt)
        else {for(i in 1:length(orgName)){
              stxt<-gsub(orgName[i], "ORG", stxt)
            }
        }
      }
      if(length(dat_en) != 0){
        if(length(dat_en) == 1)stxt<-gsub(datName, "DATE", stxt)
        else {for(i in 1:length(datName)){
              stxt<-gsub(datName[i], "DATE", stxt)
            }
        }
      }
      if(length(mon_en) != 0){
        if(length(mon_en) == 1)stxt<-gsub(monName, "MONEY", stxt)
        else {for(i in 1:length(monName)){
              stxt<-gsub(monName[i], "MONEY", stxt)
            }
        }
      }
      if(length(percent_en) != 0){
        if(length(percent_en) == 1)stxt<-gsub(percentName, "PERCENTAGE", stxt)
        else {for(i in 1:length(percentName)){
              stxt<-gsub(percentName[i], "PERCENTAGE", stxt)
            }
        }
      }
      #Replace Number
      stxt<-gsub("[[:digit:]]+", "NUM", stxt)
      
      reuters[j,125]<-stxt
    }
  }
}

#for(k in 1:9720){
#  reuters[k,125]<-as.String(reuters[k,124])
#}

#write.csv(reuters, "backup.csv", row.names = F)
write.csv(reuters, "reutersProcessed.csv", row.names = F)

asd<-reuters[,123] == ""
natext<-which(asd == TRUE)
reuters111<-reuters[,-124]
reuters111<-reuters111[-natext,]

write.csv(reuters111, "FinalProcessed.csv", row.names = F)

#同样算法处理title之后会得到很多空的title，1000个里面有394个，所以决定不处理title，应该是在选择词性的时候过于严格导致的。
#generalize the document term matrix
